{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning workshop\n",
    "\n",
    "The goal of unsupervised  learning is generally to find patterns in a dataset with minimal human input. Perhaps the data is unlabeled, and the goal could be exploration, or due to some intuition or information about the dataset there is an expected pattern and the goal is to tease it out. Perhaps the data is partially labeled, and the goal is to group unlabeled data with labeled data (this was essentially the goal of my insight project).\n",
    "\n",
    "Dimensionality reduction via decomposition also falls under the umbrella of unsupervised learning, for example matrix factorization which can be used to build recommender systems. Decomposition can also be a step towards another model, as dimensionality reduction can improve performance (not just computational!) for high-dimensional and sparse data.\n",
    "\n",
    "Anomaly detection is often described as semi-supervised learning, however it can be fully unsupervised. My understanding is that the difference lies in whether the model is initially trained on data containing outliers. I'll talk about this more later on when I show some examples.\n",
    "\n",
    "There are also unsupervised NN techniques, which I'm not going to cover due to both a lack of time and more crucially a lack of understanding. I would be super interested if somebody stepped up to give a workshop on that topic! Finally there are many more unsupervised techniques than I'm going to cover here, but I'll try to make suggestions for further reading as we go!\n",
    "\n",
    "Types of unsupervised learning:\n",
    "* Blind signal separation / Decomposition\n",
    "* Clustering\n",
    "* Anomaly detection\n",
    "* More!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "Principal component analysis hand-wavy explanation using words stolen from wikipedia: For some collection data points in n-dimensions, you can always define a best fitting line, for example with least squares. You can then define another best fitting line in a direction orthogonal to the first line. This process can be repeated to yield an orthogonal basis, where the basis vectors are called principal components. The original data can be transformed into this basis, which can be of lower dimensionality than the original data while still capturing some or all of the variance in the data.\n",
    "\n",
    "The dimensionality reduction by itself can already be very useful: Maybe you have high dimensional data and you want to be able to visualize it in a scatter plot, accepting that you will lose some information by doing so. Maybe you have some very high dimensional data (hopefully you've done some feature selection first!) that you are throwing into another model and you need to speed up training, or you believe the high dimensionality itself is causing issues (curse of dimensionality)!\n",
    "\n",
    "Anyway, let's dive into a simple example with some data we're all familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = [\"Thickness\", \"U Cell Size\", \"U Cell Shape\", \"Adhesion\", \"SE Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\",\n",
    "       \"Normal Nuclei\", \"Mitoses\", \"Class\"]\n",
    "patient_data = pd.read_csv(\"./breast-cancer-wisconsin.csv\", header=None, index_col=0, names=cols)\n",
    "patient_data = patient_data[patient_data.loc[:,\"Bare Nuclei\"] != \"?\"]\n",
    "patient_data[\"Bare Nuclei\"] = pd.to_numeric(patient_data[\"Bare Nuclei\"])\n",
    "patient_data[\"Class\"] = np.where(patient_data[\"Class\"] == 4, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X, y = patient_data.iloc[:,:-1], patient_data.iloc[:,-1]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.plot(X_pca[:,0][y==0], X_pca[:,1][y==0], 'o', color='r', label='benign')\n",
    "plt.plot(X_pca[:,0][y==1], X_pca[:,1][y==1], 'o', color='b', label='malignant')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance\n",
    "\n",
    "Explained variance is a measure of how much a given model accounts for the dispersion of a dataset. The PCA transformation will tend to lose information in this way, i.e. fail to account for all of the variance in a dataset. Looking at the explained variance is a way to determine how much information you are losing due to PCA. That is the case in the example below.\n",
    "\n",
    "On the other hand, this can also reveal that almost all of the variance in your n-dimensional data is captures by some (n-m)-dimensional PCA transformation. I don't think this necessarily means that some of your features are redundant (invite expert opinion here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other forms of PCA available in scikit-learn\n",
    "\n",
    "[documentation](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "\n",
    "The example above used \"vanilla\" PCA, which has some drawbacks:\n",
    "1. Because the dataset is centered before the transformation, it doesn't perform well for sparse matrices\n",
    "2. Entire dataset has to be stored in memory, which can be difficult for large datasets.\n",
    "3. PCA transformation gives a dense representation, i.e. output can't be sparse, even if a sparse representation might be more sensible.\n",
    "\n",
    "For sparse data, its recommended to use SVD instead of PCA, which is my next example.\n",
    "\n",
    "If sparse output is desired, sklearn's SparsePCA introduces an l1 penalty to shrink some coefficients to zero.\n",
    "\n",
    "IncrementalPCA aims to address the memory issue of PCA using partial computation so that the entire dataset is never stored in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition -- TruncatedSVD\n",
    "\n",
    "Singular value decomposition is a well known matrix decomposition method, so I won't go into the math! The \"truncated\" in TruncatedSVD is to indicate that rather than doing a full decomposition, it will be truncated after computing the k largest single values, where k is a user input. SVD doesn't have the centering requirement of PCA and thus performs better on sparse matrices. One obvious application is in NLP, where it is not unusual to have large, sparse matrices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "plt.plot(X_svd[:,0][y==0], X_svd[:,1][y==0], 'o', color='r', label='benign')\n",
    "plt.plot(X_svd[:,0][y==1], X_svd[:,1][y==1], 'o', color='b', label='malignant')\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent component analysis\n",
    "\n",
    "ICA is a technique for decomposing some \"signal\" into additive subcomponents. It determines a new set of features and coefficients, such that features in the original data can be represented as linear combinations of the new features and coefficents.\n",
    "\n",
    "In order for this to work, the new features are assumed to be non-Gaussian and statistically independent. ICA works particularly well for decomposing sound data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
    "s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n",
    "\n",
    "S = np.c_[s1, s2, s3]\n",
    "S += 0.2 * np.random.normal(size=S.shape)  # Add noise\n",
    "\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "# Mix data\n",
    "A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\n",
    "X_ica = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    "models = [S, X_ica]\n",
    "names = [\"True Sources\", \"Observations (mixed signal)\"]\n",
    "colors = [\"red\", \"steelblue\", \"orange\"]\n",
    "\n",
    "for ii, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(4, 1, ii)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "ica = FastICA(n_components=3)\n",
    "S_ = ica.fit_transform(X_ica)  # Reconstruct signals\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X_ica)  # Reconstruct signals based on orthogonal components\n",
    "\n",
    "models = [S_, H]\n",
    "names = [\"ICA recovered signals\", \"PCA recovered signals\"]\n",
    "\n",
    "for ii, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.subplot(4, 1, ii)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "One sentence explanation: Clustering will find groups or segments in the data that are similar.\n",
    "\n",
    "Two obvious questions:\n",
    "* How many groups/segments?\n",
    "* What do we mean by similar?\n",
    "\n",
    "How many groups/segments will obviously depend on the data. You might not even know ahead of time, in which case you might tune it as a hyperparameter, or use an algorithm that is agnostic to the number of clusters.\n",
    "\n",
    "There are many ways to measure the similarity, or distance, and this can also be model dependent so it will be discussed for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering builds a tree-like structure of nested clusters by successively merging or splitting clusters, depending on whether a top-down (Divisive) or bottom-up (Agglomerative) approach is taken. Divisize is more complex since some other clustering algorithm needs to be run as a subroutine to decide the splitting at each step. I couldn't actually find an implementation of divisive hierarchical clustering, but Agglomerative is available in scikit-learn and scipy.\n",
    "\n",
    "Similarity/Distance measures:\n",
    "* Euclidean\n",
    "* Cosine\n",
    "* Manhattan\n",
    "* Hamming/Jacard\n",
    "* More! Generally customizable.\n",
    "\n",
    "Clustering strategies:\n",
    "* Ward: minimizes sum of squared distances of all data points within pairs of clusters. \n",
    "* Complete: minimizes maximum distance between data points in pairs of clusters.\n",
    "* Average: minimizes the average of the distances between all data points of pairs of clusters.\n",
    "* Single/Simple: minimizes the distance between the closest data points in pairs of clusters.\n",
    "\n",
    "<img src=\"./images/clustering_affinities.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Reassign these here just in case\n",
    "X, y = patient_data.iloc[:,:-1], patient_data.iloc[:,-1]\n",
    "\n",
    "Z = linkage(X, 'ward', metric='euclidean')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "plt.xlabel(\"class\")\n",
    "plt.ylabel(\"distance\")\n",
    "dendro = dendrogram(Z, labels=np.where(y==0, \"benign\", \"malignant\"))\n",
    "ax = plt.gca()\n",
    "xlbls = ax.get_xmajorticklabels()\n",
    "for lbl in xlbls:\n",
    "    if lbl.get_text() == \"benign\":\n",
    "        lbl.set_color('r')\n",
    "    else:\n",
    "        lbl.set_color('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "\n",
    "Seperates data into n clusters by minimizing the inertia, or the in-cluster sum-of-squares:\n",
    "\n",
    "$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$\n",
    "\n",
    "Where $x_i$ are the centroids of the clusters (C) and $\\mu_j$ are the data. Since k-means relies on the cluster centroids, the distance metric is euclidean.\n",
    "\n",
    "Some draw backs:\n",
    "* Minimizing inertia assumes clusters are convex and isotropic. Will not perform well in other situations.\n",
    "* High-dimensional and/or sparse data can lead to inflated euclidean distances and poor performance. Can be alleviated to some degree by running a dimensionality reduction algorithm like PCA beforehand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "\n",
    "plt.plot(X_pca[:,0][kmeans.labels_==0], X_pca[:,1][kmeans.labels_==0], 'o', color='r', label='benign')\n",
    "plt.plot(X_pca[:,0][kmeans.labels_==1], X_pca[:,1][kmeans.labels_==1], 'o', color='b', label='malignant')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(X_pca[:,0][kmeans.labels_==y], X_pca[:,1][kmeans.labels_==y], 'o', color='k', label='correct cluster')\n",
    "plt.plot(X_pca[:,0][kmeans.labels_!=y], X_pca[:,1][kmeans.labels_!=y], 'o', color='g', label='wrong cluster')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "DBSCAN treats clusters as areas of high density that are seperated by areas of low density. To achieve this, it takes at least two parameters:\n",
    "\n",
    "1. eps = the maximum distance between two points for them to be considered as in the same neighbourhood\n",
    "2. min_samples = the minimum number of neighbouring points for a point to be considered a \"core\" point\n",
    "\n",
    "The algorithm proceeds by first identifying the core points using eps/min_samples, and building clusters outwards from those core points. Points that aren't in the neighbourhood of any clusters are left out of the clustering and marked as noise.\n",
    "\n",
    "It has major computational advantages over hierarchical clustering as it doesn't need to know pair-wise distances between all points in the dataset, only distances between core points and their neighbours are important. It also has a flexibility advantage over k-means, clusters can be any shape/size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=5, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "plt.plot(X_pca[:,0][labels==0], X_pca[:,1][labels==0], 'o', color='r', label='benign')\n",
    "plt.plot(X_pca[:,0][labels==1], X_pca[:,1][labels==1], 'o', color='b', label='malignant')\n",
    "plt.plot(X_pca[:,0][labels==-1], X_pca[:,1][labels==-1], 'o', color='g', label='noise')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation for other clustering algorithms:\n",
    "\n",
    "There are many other options for clustering algorithms, I couldn't cover them all here! Picking the right clustering algorithm is generally going to depend on the form of your data. For docs on the scikit-learn implementations, use this link:\n",
    "\n",
    "[documentation](https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating clustering output\n",
    "\n",
    "**Cophenetic correlation coefficient**: Correlation between the array of pair-wise distances between original points, and array of inter-cluster distances. A measure of how well the clustering did at clustering \"similar\" data points. Not implemented in scikit-learn!\n",
    "\n",
    "**Silhouette score**: Calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample, where b is the nearest cluster that the sample is not a part of. For a given sample: \n",
    "\n",
    "score = (b - a)/max(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Note: Using the hierarchical clustering output here\n",
    "print(\"Cophenetic correlation coefficient of hierararchical clustering: {}\".format(cophenet(Z, pdist(X))[0]))\n",
    "\n",
    "# Note: Using the k-means clustering output here\n",
    "print(\"Silhouette score of k-means clustering: {}\".format(silhouette_score(X, kmeans.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "Anomaly detection can be broadly split into two categories.\n",
    "\n",
    "1. **Unsupervised AKA Outlier  detection** -- training data is contaminated with outliers, algorithms try to fit dense regions in the training data to identify outliers.\n",
    "2. **(semi) Supervised AKA Novelty detection** -- training data is **not** contaminated with outliers, algorithms should identify whether new observations are consistent with the training data or are outliers.\n",
    "\n",
    "In either case the goal is anomaly detection, however the unsupervised version has the limitation that the outliers cannot form a dense cluster as available algorithms assume that outliers are located in low density regions. Novelty detection does not suffer from this constraint, outliers can form a dense cluster **as long as they occur in a low density region in the training data**.\n",
    "\n",
    "I'm going to treat my examples as if they are unsupervised, but my toy data will actually be labeled so we can get an idea of how well each algorithm is doing. Let's start by creating a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_inliers = 0.3 * np.random.randn(100, 2)\n",
    "X_inliers = np.concatenate([X_inliers + 2, X_inliers - 2], axis=0)\n",
    "\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "X_all = np.concatenate([X_inliers, X_outliers], axis=0)\n",
    "y_true = np.ones(len(X_all), dtype=int)\n",
    "y_true[-len(X_outliers):] = -1\n",
    "\n",
    "plt.scatter(X_inliers[:,0], X_inliers[:,1], color='k', label='inliers')\n",
    "plt.scatter(X_outliers[:,0], X_outliers[:,1], color='r', label='outliers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One class SVM\n",
    "\n",
    "We're already familiar with SVM from a previous workshop. This is a reformulation that, rather than trying to \"split\" our data into two or more classes, tries to form a boundary identifying a single class. Anything outside the boundary is an outlier!\n",
    "\n",
    "Parameters tuned in the example:\n",
    "1. **kernel** -- for convex clusters as in our toy dataset RBF is probably the most sensible choice\n",
    "2. **gamma** -- a scale that will control the influence of a single training data point\n",
    "3. **nu** -- upper bound on the allowed number of training errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "svm = OneClassSVM(nu=0.12, kernel=\"rbf\", gamma=0.25)\n",
    "y_pred = svm.fit_predict(X_all)\n",
    "y_pred_in, y_pred_out = y_pred[:len(X_inliers)], y_pred[len(X_inliers):]\n",
    "y_true_in, y_true_out = y_true[:len(X_inliers)], y_true[len(X_inliers):]\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "b1 = plt.scatter(X_all[:,0][y_pred == y_true], X_all[:,1][y_pred == y_true], color='k', label='correct')\n",
    "b2 = plt.scatter(X_inliers[:,0][y_pred_in != y_true_in], X_inliers[:,1][y_pred_in != y_true_in], \n",
    "            color='g', label=\"misclassified inliers\")\n",
    "b3 = plt.scatter(X_outliers[:,0][y_pred_out != y_true_out], X_outliers[:,1][y_pred_out != y_true_out], \n",
    "            color='b', label=\"misclassified outliers\")\n",
    "plt.legend([a.collections[0], b1, b2, b3], [\"Decision boundary\", \"correct\", \"misclassified inliers\", \"misclassified outliers\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local outlier factor\n",
    "\n",
    "Searches for sample that have a substantially lower density than their neighbours by measuring the local density deviation of a given data point with respect to its neighbours.\n",
    "\n",
    "The \"LOF score\" of a sample is equal to the ratio of the average local density of its k-nearest neighbors, and its own local density. A normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "y_pred = lof.fit_predict(X_all)\n",
    "n_errors = (y_pred != y_true).sum()\n",
    "print(\"Prediction errors: {}\".format(n_errors))\n",
    "\n",
    "X_scores = lof.negative_outlier_factor_\n",
    "\n",
    "# Plot the original data with red circles of radius proportional to the model prediction\n",
    "plt.title(\"Local Outlier Factor (LOF)\")\n",
    "plt.scatter(X_all[:, 0], X_all[:, 1], color='k', s=3., label='Data points')\n",
    "radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
    "plt.scatter(X_all[:, 0], X_all[:, 1], s=1000 * radius, edgecolors='r',\n",
    "            facecolors='none', label='Outlier scores')\n",
    "plt.show()\n",
    "\n",
    "# Plot the original data with misclassified data points highlighted\n",
    "y_pred_in, y_pred_out = y_pred[:len(X_inliers)], y_pred[len(X_inliers):]\n",
    "y_true_in, y_true_out = y_true[:len(X_inliers)], y_true[len(X_inliers):]\n",
    "plt.scatter(X_all[:,0][y_pred == y_true], X_all[:,1][y_pred == y_true], color='k', label='correct')\n",
    "plt.scatter(X_inliers[:,0][y_pred_in != y_true_in], X_inliers[:,1][y_pred_in != y_true_in], \n",
    "            color='g', label='misclassified inliers')\n",
    "plt.scatter(X_outliers[:,0][y_pred_out != y_true_out], X_outliers[:,1][y_pred_out != y_true_out], \n",
    "            color='b', label='misclassified outliers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolated Forest\n",
    "\n",
    "The Isolated Forest algorithm ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "The number of splittings required to isolate a sample in an individual tree is taken as a path length, and the average path length over many random trees is used as a measure of the \"normality\" of the sample. Random partitioning of the dataset tends to introduce shorter path lengths for anomalies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isof = IsolationForest(max_samples=200)\n",
    "y_pred = isof.fit_predict(X_all)\n",
    "n_errors = (y_pred != y_true).sum()\n",
    "print(\"Prediction errors: {}\".format(n_errors))\n",
    "y_pred_out = isof.predict(X_outliers)\n",
    "y_pred_in = isof.predict(X_inliers)\n",
    "\n",
    "plt.scatter(X_all[:,0][y_pred == y_true], X_all[:,1][y_pred == y_true], c='k', label=\"correct\")\n",
    "plt.scatter(X_inliers[:,0][y_pred_in != y_true_in], X_inliers[:,1][y_pred_in != y_true_in], c='g', label=\"misclassified inlier\")\n",
    "plt.scatter(X_outliers[:,0][y_pred_out != y_true_out], X_outliers[:,1][y_pred_out != y_true_out], c='b', label=\"misclassified outlier\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exploration\n",
    "\n",
    "Some topics I didn't cover today that I think would be interesting and worthwhile to look at:\n",
    "* Non-negative matrix factorization\n",
    "* Autoencoders\n",
    "* Generative Adversarial Networks\n",
    "* Self organizing maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
